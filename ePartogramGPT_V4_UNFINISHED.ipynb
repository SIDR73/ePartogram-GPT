{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEEsQbSoSmhm"
      },
      "source": [
        "**ePartogram-GPT using Zephyr 7B Beta and Pytorch NLP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxu0B_OiSycH"
      },
      "source": [
        "**System Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "LB2Q9pgs3y2k"
      },
      "outputs": [],
      "source": [
        "#install required packages\n",
        "%%capture\n",
        "!pip install -q transformers peft accelerate bitsandbytes safetensors sentencepiece streamlit chromadb langchain langchain-community langchain-core langchain-huggingface sentence-transformers gradio pypdf SpeechRecognition\n",
        "!pip install haystack-ai transformers accelerate bitsandbytes sentence_transformers bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mhmVRe6bcFnf"
      },
      "outputs": [],
      "source": [
        "# import audio packages\n",
        "%%capture\n",
        "!apt-get update\n",
        "!apt-get install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
        "!pip install pyaudio\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8KPiKYP-Yz_V"
      },
      "outputs": [],
      "source": [
        "# fixing unicode error in google colab\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b08wbHdz3y2m"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "import os\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "import csv\n",
        "\n",
        "import chromadb\n",
        "#from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Engineering Packages\n",
        "from haystack import Pipeline\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.generators import HuggingFaceLocalGenerator"
      ],
      "metadata": {
        "id": "z5FF_UiWqUnl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R-8o_oNkEl0k"
      },
      "outputs": [],
      "source": [
        "# Speech Recognition and Chat History Packages\n",
        "import speech_recognition as sr\n",
        "import re\n",
        "import csv\n",
        "from datetime import datetime, timedelta\n",
        "# from pocketsphinx import LiveSpeech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNGUXhUqYNT4"
      },
      "source": [
        "**Define and Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def speech_to_text():\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Listening...\")\n",
        "        audio = recognizer.listen(source)\n",
        "\n",
        "    try:\n",
        "        text = recognizer.recognize_google(audio, language=\"en-US\")\n",
        "        return text\n",
        "    except sr.UnknownValueError:\n",
        "        return \"Speech recognition could not understand audio\"\n",
        "    except sr.RequestError as e:\n",
        "        return f\"Could not request results from speech recognition service; {e}\"\n",
        "\n",
        "def process_audio(audio_file):\n",
        "    if audio_file is not None:\n",
        "        try:\n",
        "            recognizer = sr.Recognizer()\n",
        "            with sr.AudioFile(audio_file) as source:\n",
        "                audio_data = recognizer.record(source)\n",
        "            text = recognizer.recognize_google(audio_data)\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            return f\"Error in speech recognition: {str(e)}\"\n",
        "    return \"No audio input received\""
      ],
      "metadata": {
        "id": "qr9BMSTOooli"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kKkVcgAv8frU"
      },
      "outputs": [],
      "source": [
        "# specify model huggingface mode name\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# function for loading 4-bit quantized model\n",
        "def load_quantized_model(model_name: str):\n",
        "    \"\"\"\n",
        "    :param model_name: Name or path of the model to be loaded.\n",
        "    :return: Loaded quantized model.\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2C0DVleS8iwj"
      },
      "outputs": [],
      "source": [
        "# function for initializing tokenizer\n",
        "def initialize_tokenizer(model_name: str):\n",
        "    \"\"\"\n",
        "    Initialize the tokenizer with the specified model_name.\n",
        "\n",
        "    :param model_name: Name or path of the model for tokenizer initialization.\n",
        "    :return: Initialized tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.bos_token_id = 1  # Set beginning of sentence token id\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EQP-tdh88qiv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825,
          "referenced_widgets": [
            "46e1c048c8ae4595bea86216f6605530",
            "c9fe7a63aed5456792d74e5d80a0ce0b",
            "468b86afe8cd4f859533c900fd1adfba",
            "febe79484c9d4ce49e04e7f5b6dbc785",
            "8de3369513774bd58d0544c41a76516b",
            "a04424855c66446d8fa50fb26b658950",
            "cdc7ed5c175340a1a9e4cbf4e0be36f9",
            "e60c66fce85245679dc74aa4518727f9",
            "a0ab49cdc9404db5b370c11b37ad8d14",
            "bc69c9b08036457f841252b0934c78c6",
            "23193645ab5d49ed84a96042cfd7164e"
          ]
        },
        "outputId": "d4bff45e-b490-42e7-fd6c-ba8992c8cd75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46e1c048c8ae4595bea86216f6605530"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b06283efd10a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -U bitsandbytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_quantized_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# initialize tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-1af36995dc36>\u001b[0m in \u001b[0;36mload_quantized_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3657\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3658\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3659\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mbnb_multibackend_is_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_bitsandbytes_multi_backend_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mvalidate_bnb_backend_availability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"from_tf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"from_flax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/bitsandbytes.py\u001b[0m in \u001b[0;36mvalidate_bnb_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_bitsandbytes_multi_backend_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_validate_bnb_multi_backend_availability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_validate_bnb_cuda_backend_availability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/bitsandbytes.py\u001b[0m in \u001b[0;36m_validate_bnb_cuda_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes\n",
        "# load model\n",
        "model = load_quantized_model(model_name)\n",
        "\n",
        "# initialize tokenizer\n",
        "tokenizer = initialize_tokenizer(model_name)\n",
        "\n",
        "# specify stop token ids\n",
        "stop_token_ids = [0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVdYNvptU5Wu"
      },
      "outputs": [],
      "source": [
        "# mount google drive and specify folder path\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/My_PDFs_AIML'\n",
        "!ls '/content/drive/MyDrive/Colab Notebooks/My_PDFs_AIML'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieval Augmented Generation (RAG)**"
      ],
      "metadata": {
        "id": "9xcN6L5Cs-0W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GlWVUlP8AiHJ"
      },
      "outputs": [],
      "source": [
        "# load pdf files\n",
        "loader = PyPDFDirectoryLoader(folder_path)\n",
        "documents = loader.load()\n",
        "print(documents)\n",
        "\n",
        "# split the documents in small chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100) #Change the chunk_size and chunk_overlap as needed\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "# specify embedding model (using huggingface sentence transformer)\n",
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "embeddings = HuggingFaceEmbeddings(model_name = embedding_model_name, model_kwargs = model_kwargs) #TO DO\n",
        "\n",
        "#embed document chunks\n",
        "vectordb = Chroma.from_documents(documents = all_splits, embedding = embeddings, persist_directory=\"chroma_db\") #TO DO\n",
        "\n",
        "# specify the retriever\n",
        "retriever = vectordb.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    top_k=5,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "# max_length = 2048\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "I3SSO_gjcYFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt Engineering**"
      ],
      "metadata": {
        "id": "0uPBOWfMsY7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"<|system|>\n",
        "You are an AI assistant specialized in analyzing and interpreting medical documents. Provide accurate and helpful information based on the given context and question. Always prioritize patient safety and adhere to medical best practices in your responses.\n",
        "</s>\n",
        "<|user|>\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "ztGoTow7cjoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    combine_docs_chain_kwargs={\"prompt\": PROMPT}\n",
        ")"
      ],
      "metadata": {
        "id": "dtuikTDlc6R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLP Processing of Chat Input and AI Output**"
      ],
      "metadata": {
        "id": "rXn-cTENsnD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_relevant_info(text):\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Define the entities we want to extract\n",
        "    entities = {\n",
        "        'Companion': None,\n",
        "        'Pain Relief': None,\n",
        "        'Oral fluid': None,\n",
        "        'Posture': None,\n",
        "        'Baseline FHR': None,\n",
        "        'FHR Decceleration': None,\n",
        "        'Amniotic fluid': None,\n",
        "        'Fetal position': None,\n",
        "        'Caput': None,\n",
        "        'Moulding': None,\n",
        "        'Pulse': None,\n",
        "        'Systolic BP': None,\n",
        "        'Diastolic BP': None,\n",
        "        'Temperature': None,\n",
        "        'Urine': None,\n",
        "        'Contractions per 10 min': None,\n",
        "        'Duration of contractions': None,\n",
        "        'Cervix': None,\n",
        "        'Descent': None,\n",
        "        'Medication': None,\n",
        "        'Assessment': None,\n",
        "        'Plan': None\n",
        "    }\n",
        "\n",
        "    extracted_info = {}\n",
        "    for key, pattern in entities.items():\n",
        "        match = re.search(pattern, text, re.IGNORECASE)\n",
        "        extracted_info[key] = match.group(1).strip() if match else None\n",
        "\n",
        "    return entities"
      ],
      "metadata": {
        "id": "LClVS7qjBDhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_chat_to_csv(chat_history: list, filename: str = '/content/drive/MyDrive/digital_lcg.csv'):\n",
        "    # Define parameters to LCG\n",
        "    fields = ['Timestamp', 'Companion', 'Pain Relief', 'Oral fluid', 'Posture', 'Baseline FHR',\n",
        "              'FHR Decceleration', 'Amniotic fluid', 'Fetal position', 'Caput', 'Moulding',\n",
        "              'Pulse', 'Systolic BP', 'Diastolic BP', 'Temperature', 'Urine',\n",
        "              'Contractions per 10 min', 'Duration of contractions', 'Cervix', 'Descent',\n",
        "              'Medication', 'Assessment', 'Plan']\n",
        "\n",
        "    # Read existing data from digital LCG\n",
        "    try:\n",
        "        with open(filename, 'r', newline='', encoding='utf-8') as file:\n",
        "            existing_data = list(csv.DictReader(file))\n",
        "    except FileNotFoundError:\n",
        "        existing_data = []\n",
        "\n",
        "    # Extract data from chat history\n",
        "    new_entries = []\n",
        "    for query, response in chat_history:\n",
        "        info = extract_relevant_info(response)\n",
        "        new_entry = {field: '' for field in fields}\n",
        "        new_entry['Timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        new_entry.update(info)\n",
        "        new_entries.append(new_entry)\n",
        "\n",
        "    # Write all data back to the CSV\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=fields)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(existing_data + new_entries)"
      ],
      "metadata": {
        "id": "Sto9YUxhs21W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UI Development**"
      ],
      "metadata": {
        "id": "SWvnOlBasbgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(query: str, chat_history: list):\n",
        "    result = qa_chain({\"question\": query, \"chat_history\": chat_history})\n",
        "    return result[\"answer\"]"
      ],
      "metadata": {
        "id": "Kx3RtyM2c-ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_conversation(message, history):\n",
        "    try:\n",
        "        response = get_response(message, history)\n",
        "        assistant_response = response.split(\"<|assistant|>\")[-1].strip()\n",
        "        formatted_response = f\"Question: {message}\\n\\nHelpful Answer: {assistant_response}\"\n",
        "        history.append((message, formatted_response))\n",
        "\n",
        "        save_to_csv_message = message\n",
        "\n",
        "        # Segment the message into the WHO Labor Care Guide\n",
        "        save_chat_to_csv(save_to_csv_message)\n",
        "        return \"\", history\n",
        "    except Exception as e:\n",
        "        error_message = f\"Question: {message}\\n\\nHelpful Answer: An error occurred: {str(e)}\"\n",
        "        history.append((message, error_message))\n",
        "        return \"\", history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"ePartogram-GPT\")\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"Chat History\")\n",
        "    msg = gr.Textbox(label=\"Enter your question\", placeholder=\"Type your question here...\")\n",
        "    audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Or speak your question\")\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "    msg.submit(create_conversation, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "    msg.submit(lambda: \"\", inputs=[], outputs=[msg])\n",
        "    audio_input.change(process_audio, inputs=[audio_input], outputs=[msg])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "agSW1jXue2Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Cases**"
      ],
      "metadata": {
        "id": "FUvIqGgSsfiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Query 1:** Mrs. A, Para 0+0 presents in labor at 39 weeks , uneventful ANC, Contractions 1/10 each 20 seconds, Pulse 88, BP 130/80, FHR 140/min, no decelerations, Head all up, Cervix mostly effaced, membranes intact, 4 cm dilatated. Assess the state of the patient."
      ],
      "metadata": {
        "id": "5xhSrFN0vQBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST QUERY 1\n",
        "query = \"Mrs. A, Para 0+0 presents in labor at 39 weeks , uneventful ANC, Contractions 1/10 each 20 seconds, Pulse 88, BP 130/80, FHR 140/min, no decelerations, Head all up, Cervix mostly effaced, membranes intact, 4 cm dilatated. Assess the state of the patient.\"\n",
        "chat_history = []  # Initialize an empty chat history\n",
        "response = get_response(query, chat_history)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "hLZnJMhJdCmc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual AI Result\n",
        "response_filtered = response.split(\"<|assistant|>\")[-1].strip()\n",
        "print(response_filtered)"
      ],
      "metadata": {
        "id": "45MHnNI0efX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Query 2:** Mrs. A, Para 0+0 presents in labor at 39 weeks , uneventful ANC, Contractions 1/10 each 20 seconds, Pulse 88, BP 130/80, FHR 140/min, no decelerations, Head all up, Cervix mostly effaced, membranes intact, 4 cm dilated. Provide a plan of care."
      ],
      "metadata": {
        "id": "FLcyMrVXu8r3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST QUERY 2:\n",
        "query = \"Mrs. A, Para 0+0 presents in labor at 39 weeks , uneventful ANC, Contractions 1/10 each 20 seconds, Pulse 88, BP 130/80, FHR 140/min, no decelerations, Head all up, Cervix mostly effaced, membranes intact, 4 cm dilated. Provide a plan of care.\"\n",
        "chat_history = []  # Initialize an empty chat history\n",
        "response = get_response(query, chat_history)\n",
        "print(response)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SryuL6akq2xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual AI Result\n",
        "response_filtered = response.split(\"<|assistant|>\")[-1].strip()\n",
        "print(response_filtered)"
      ],
      "metadata": {
        "id": "AOa6MxPVuxBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Query 3:** Mrs. C, 20 years old Para 0+1, TOP at 12 weeks 2 years ago, iron deficiency anemia on treatment with iron. Last hemoglobin 10.5g. Presents at 39 weeks with labor pains for 4 hours, Examination  revealed active labor, ARM done, clear amniotic fluid, normal progress, Buscopan given, instructed her sister to walk with Mrs. C. Comment on plan"
      ],
      "metadata": {
        "id": "7u-ITTlXsJC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Mrs. C, 20 years old Para 0+1, TOP at 12 weeks 2 years ago, iron deficiency anemia on treatment with iron. Last hemoglobin 10.5g. Presents at 39 weeks with labor pains for 4 hours, Examination  revealed active labor, ARM done, clear amniotic fluid, normal progress, Buscopan given, instructed her sister to walk with Mrs. C. Comment on plan\"\n",
        "chat_history = []  # Initialize an empty chat history\n",
        "response = get_response(query, chat_history)\n",
        "print(response)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fkC3yLrTsOfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual AI Result\n",
        "response_filtered = response.split(\"<|assistant|>\")[-1].strip()\n",
        "print(response_filtered)"
      ],
      "metadata": {
        "id": "t7h5ie4VtSwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Query 4:** (Bony Obstruction) A 28-year-old woman presents to the labor and delivery unit at 41 weeks gestation with regular contractions. After 18 hours of labor, she has made minimal cervical progress despite adequate uterine contractions. The fetus remains high in the pelvis, and the cervix is 6 cm dilated. Assess state of patient."
      ],
      "metadata": {
        "id": "nptWKbq2xemw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"A 28-year-old woman presents to the labor and delivery unit at 41 weeks gestation with regular contractions. After 18 hours of labor, she has made minimal cervical progress despite adequate uterine contractions. The fetus remains high in the pelvis, and the cervix is 6 cm dilated. Assess state of patient.\"\n",
        "chat_history = []  # Initialize an empty chat history\n",
        "response = get_response(query, chat_history)\n",
        "print(response)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OkSR87OVxK-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual AI Result\n",
        "response_filtered = response.split(\"<|assistant|>\")[-1].strip()\n",
        "print(response_filtered)"
      ],
      "metadata": {
        "id": "F9J5U1Oz-8DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Issue with Test Query 4: Oxytocin would actually make things worse, so we need to add specific RAG rules here"
      ],
      "metadata": {
        "id": "1mF6CpL7MN-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Query 5:**"
      ],
      "metadata": {
        "id": "FhaDSmhd_uCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\n",
        "chat_history = []  # Initialize an empty chat history\n",
        "response = get_response(query, chat_history)\n",
        "response_filtered = response.split(\"<|assistant|>\")[-1].strip()\n",
        "print(response_filtered)"
      ],
      "metadata": {
        "id": "pVUSIzRq_tdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\n",
        "chat_history = []  # Initialize an empty chat history\n",
        "response = get_response(query, chat_history)\n",
        "response_filtered = response.split(\"<|assistant|>\")[-1].strip()\n",
        "print(response_filtered)"
      ],
      "metadata": {
        "id": "w7prUyo8_6Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\n",
        "chat_history = []  # Initialize an empty chat history\n",
        "response = get_response(query, chat_history)\n",
        "response_filtered = response.split(\"<|assistant|>\")[-1].strip()\n",
        "print(response_filtered)"
      ],
      "metadata": {
        "id": "SRgL41Ei_7C6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46e1c048c8ae4595bea86216f6605530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9fe7a63aed5456792d74e5d80a0ce0b",
              "IPY_MODEL_468b86afe8cd4f859533c900fd1adfba",
              "IPY_MODEL_febe79484c9d4ce49e04e7f5b6dbc785"
            ],
            "layout": "IPY_MODEL_8de3369513774bd58d0544c41a76516b"
          }
        },
        "c9fe7a63aed5456792d74e5d80a0ce0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a04424855c66446d8fa50fb26b658950",
            "placeholder": "​",
            "style": "IPY_MODEL_cdc7ed5c175340a1a9e4cbf4e0be36f9",
            "value": "config.json: 100%"
          }
        },
        "468b86afe8cd4f859533c900fd1adfba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e60c66fce85245679dc74aa4518727f9",
            "max": 638,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0ab49cdc9404db5b370c11b37ad8d14",
            "value": 638
          }
        },
        "febe79484c9d4ce49e04e7f5b6dbc785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc69c9b08036457f841252b0934c78c6",
            "placeholder": "​",
            "style": "IPY_MODEL_23193645ab5d49ed84a96042cfd7164e",
            "value": " 638/638 [00:00&lt;00:00, 8.18kB/s]"
          }
        },
        "8de3369513774bd58d0544c41a76516b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a04424855c66446d8fa50fb26b658950": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdc7ed5c175340a1a9e4cbf4e0be36f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e60c66fce85245679dc74aa4518727f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0ab49cdc9404db5b370c11b37ad8d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc69c9b08036457f841252b0934c78c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23193645ab5d49ed84a96042cfd7164e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}